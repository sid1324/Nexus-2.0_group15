{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVoU7VrzGzxn",
        "outputId": "8d2f9c5b-9b10-4655-8299-32042d2e2838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from openai==0.28) (4.61.2)\n",
            "Requirement already satisfied: aiohttp in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from openai==0.28) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
            "Requirement already satisfied: numpy in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (1.25.2)\n",
            "Requirement already satisfied: tiktoken in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from tiktoken) (2022.7.9)\n",
            "Requirement already satisfied: requests>=2.26.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Requirement already satisfied: Gradio in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (5.4.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (22.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (3.5.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.115.4)\n",
            "Requirement already satisfied: ffmpy in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (1.4.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (2.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (10.0.1)\n",
            "Requirement already satisfied: pydantic>=2.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (6.0)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.7.2)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.41.2)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from Gradio) (0.32.0)\n",
            "Requirement already satisfied: fsspec in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from gradio-client==1.4.2->Gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from gradio-client==1.4.2->Gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from anyio<5.0,>=3.0->Gradio) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from anyio<5.0,>=3.0->Gradio) (1.2.0)\n",
            "Requirement already satisfied: certifi in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from httpx>=0.24.1->Gradio) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from httpx>=0.24.1->Gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->Gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.25.1->Gradio) (3.9.0)\n",
            "Requirement already satisfied: requests in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.25.1->Gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.25.1->Gradio) (4.61.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from pandas<3.0,>=1.0->Gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from pandas<3.0,>=1.0->Gradio) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from pandas<3.0,>=1.0->Gradio) (2023.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from pydantic>=2.0->Gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from pydantic>=2.0->Gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from typer<1.0,>=0.12->Gradio) (8.0.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from typer<1.0,>=0.12->Gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from typer<1.0,>=0.12->Gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->Gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->Gradio) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->Gradio) (2.15.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.1->Gradio) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.25.1->Gradio) (1.26.16)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->Gradio) (0.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install openai==0.28\n",
        "!pip3 install numpy\n",
        "!pip3 install tiktoken\n",
        "!pip3 install Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WysuPFYQHfCq"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import tiktoken\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e225JxvyyOtZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_ofhSD5BII1W"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO8Q9d0b4UGH",
        "outputId": "b2c22b4c-75b0-47f4-d997-e837f8eb9c20"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySonCJu2I1KD",
        "outputId": "3fd586af-97f8-4ed2-dcce-1d4f29e64772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON decode error for cell 'messages': Expecting value: line 1 column 1 (char 0)\n",
            "JSON decode error for cell '{“messages\":[{\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"What font weight should I use for different text types on the dashboard?\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"For Euclid’s dashboard:\\n\\n- **Headers**: Use a bold font weight (700) to make section titles stand out.\\n- **Subheaders**: Use a semi-bold weight (600) for category labels.\\n- **Body Text**: Use a regular weight (400) to maintain readability and consistency.\\n\\nThis helps to establish a clear hierarchy while maintaining a clean and professional appearance.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"What style should I use for a tooltip displaying transaction details?\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"For a tooltip in Euclid’s platform, use a dark background with light text for readability. Here’s an example:\\n\\n```javascript\\nimport React from 'react';\\n\\nconst Tooltip = ({ content }) => (\\n  <div className='bg-gray-800 text-white text-sm p-2 rounded shadow-md'>\\n    {content}\\n  </div>\\n);\\n\\nexport default Tooltip;\\n```\\n\\nThis tooltip design provides a clear, accessible display of transaction details with a subtle shadow for emphasis.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"What is the best way to display blockchain network activity with Euclid styling?\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"Use a line chart or area chart with Euclid's green color for activity peaks. Here’s an example setup with Chart.js:\\n\\n```javascript\\nimport { Line } from 'react-chartjs-2';\\n\\nconst NetworkActivityChart = ({ data }) => (\\n  <div className='bg-gray-100 p-4 rounded-md'>\\n    <Line data={{\\n      labels: data.labels,\\n      datasets: [{\\n        label: 'Network Activity',\\n        data: data.values,\\n        borderColor: '#28a745',\\n        backgroundColor: 'rgba(40, 167, 69, 0.2)',\\n      }],\\n    }} />\\n  </div>\\n);\\n\\nexport default NetworkActivityChart;\\n```\\n\\nThis chart uses Euclid’s green for line and fill color, with a light grey background for readability.\"\n",
            "    }]}': Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
            "JSON decode error for cell '{\"messages\":[{{\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"What animation can I use to show transitions between pages?\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"For transitions between pages, a fade effect or slide-in animation works well and keeps the interface smooth. Using React Transition Group, here’s a fade animation:\\n\\n```javascript\\nimport { CSSTransition } from 'react-transition-group';\\n\\nconst FadeTransition = ({ children, inProp }) => (\\n  <CSSTransition in={inProp} timeout={300} classNames='fade' unmountOnExit>\\n    {children}\\n  </CSSTransition>\\n);\\n\\nexport default FadeTransition;\\n\\n/* CSS */\\n.fade-enter { opacity: 0; }\\n.fade-enter-active { opacity: 1; transition: opacity 300ms; }\\n.fade-exit { opacity: 1; }\\n.fade-exit-active { opacity: 0; transition: opacity 300ms; }\\n```\\n\\nThis creates a smooth fade-in and fade-out effect, providing a sleek transition for Euclid’s interface.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"How should I style a modal for viewing detailed blockchain transaction data?\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"For the modal, use a semi-transparent dark background to focus attention on the modal content. Here’s an example:\\n\\n```javascript\\nimport React from 'react';\\n\\nconst TransactionModal = ({ transaction, onClose }) => (\\n  <div className='fixed inset-0 bg-gray-900 bg-opacity-75 flex justify-center items-center'>\\n    <div className='bg-white p-6 rounded-md w-1/2'>\\n      <h3 className='text-green-500 font-bold'>Transaction Details</h3>\\n      <p>ID: {transaction.id}</p>\\n      <p>Status: {transaction.status}</p>\\n      <button onClick={onClose} className='mt-4 bg-green-600 text-white px-4 py-2 rounded'>Close</button>\\n    </div>\\n  </div>\\n);\\n\\nexport default TransactionModal;\\n```\\n\\nThis modal includes Euclid’s green accents and a dark background overlay to focus user attention on the transaction details.\"\n",
            "    }}]}': Expecting property name enclosed in double quotes: line 1 column 15 (char 14)\n",
            "JSON decode error for cell '{\"messages\":[{\"role\":\"user\",\"content\":\"Using Nexus 2.0, create a navbar component for Euclid's website that includes links for 'Home,' 'About Us,' 'Services,' and 'Contact.'\"},{\"role\":\"assistant\",\"content\":\"```jsx\\nimport { useButton, Ripple, Spinner } from '@nexus-ds/react';\\n\\nexport default function EuclidNavbar() {\\n return (\\n <Navbar isBordered variant=\"sticky\">\\n <Navbar.Brand>\\n <Link href=\"/\">Euclid</Link>\\n </Navbar.Brand>\\n <Navbar.Content>\\n <Link href=\"/about\">About Us</Link>\\n <Link href=\"/services\">Services</Link>\\n <Link href=\"/contact\">Contact</Link>\\n </Navbar.Content>\\n </Navbar>\\n );\\n}\\n```\"}]}': Expecting ',' delimiter: line 1 column 363 (char 362)\n",
            "JSON decode error for cell '{\"messages\":[{\"role\":\"user\",\"content\":\"Generate a footer component for Euclid’s website using Nexus 2.0 with links for 'Privacy Policy,' 'Terms of Service,' and 'Contact Support.'\"},{\"role\":\"assistant\",\"content\":\"```jsx\\nimport { useButton, Ripple, Spinner } from '@nexus-ds/react';\\n\\nexport default function EuclidFooter() {\\n  return (\\n    <Footer>\\n      <Footer.Content>\\n        <Text small>© 2024 Euclid, All Rights Reserved.</Text>\\n        <Link href=\"/privacy\">Privacy Policy</Link>\\n        <Link href=\"/terms\">Terms of Service</Link>\\n        <Link href=\"/support\">Contact Support</Link>\\n      </Footer.Content>\\n    </Footer>\\n  );\\n}\\n```\"}]}': Expecting ',' delimiter: line 1 column 463 (char 462)\n",
            "JSON decode error for cell '{\"messages\":[{\"role\":\"user\",\"content\":\"Create a form component in Nexus 2.0 to allow users to input their name, email, and message for a 'Contact Us' page on Euclid’s website.\"},{\"role\":\"assistant\",\"content\":\"```jsx\\nimport { Input, Textarea, Button, Spacer } from '@nexus-ds/react';\\n\\nexport default function ContactForm() {\\n  return (\\n    <form>\\n      <Input labelPlaceholder=\"Name\" fullWidth />\\n      <Spacer y={1} />\\n      <Input labelPlaceholder=\"Email\" fullWidth />\\n      <Spacer y={1} />\\n      <Textarea labelPlaceholder=\"Message\" fullWidth />\\n      <Spacer y={1.5} />\\n      <Button auto>Submit</Button>\\n    </form>\\n  );\\n}\\n```\"}]}': Expecting ',' delimiter: line 1 column 384 (char 383)\n"
          ]
        }
      ],
      "source": [
        "#Load CSV data in\n",
        "csv_file_path = r'Dataset_Nexus.csv'\n",
        "cleaned_data = []\n",
        "\n",
        "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        for cell in row:\n",
        "            try:\n",
        "                # Replace square brackets and inner double quotes that are problematic\n",
        "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
        "\n",
        "                # Load each cell as a JSON object\n",
        "                cell_json = json.loads(cell)\n",
        "\n",
        "                # Now that the content is clean, append to cleaned_data list\n",
        "                cleaned_data.append(cell_json)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
        "\n",
        "jsonl_file_path = r'C:\\Users\\91982\\Downloads\\gpt-finetune\\gpt-finetune\\fitness-json.jsonl'\n",
        "# Write cleaned data to a JSONL file\n",
        "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
        "    for item in cleaned_data:\n",
        "        jsonl_file.write(json.dumps(item) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NNdePOiKuKW",
        "outputId": "80e60895-23e1-4f53-e499-8116dd5bf4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num examples: 13\n",
            "First example:\n",
            "{'role': 'user', 'content': 'What color scheme would work well for a blockchain analytics dashboard?'}\n",
            "{'role': 'assistant', 'content': 'For a blockchain analytics dashboard under the Euclid branding, I recommend using a combination of green for sustainable tech representation and dark grey for a modern and secure feel. You can use the green from Euclid’s logo for highlights and accents, while the dark grey serves as a background for contrast. This will give a professional and trustworthy vibe to your dashboard, emphasizing security and sustainability.'}\n",
            "{'role': 'user', 'content': 'How can I style buttons for Euclid’s platform?'}\n",
            "{'role': 'assistant', 'content': \"To style buttons for Euclid’s platform, you can use React components along with Nexus Design System’s Button and Ripple components for a dynamic look.\\n\\nHere’s a custom implementation using the `useButton` hook:\\n\\n```javascript\\nimport { forwardRef } from 'react';\\nimport { useButton, Ripple, Spinner } from '@nexus-ds/react';\\n\\nconst EuclidButton = forwardRef((props, ref) => {\\n  const {\\n    domRef,\\n    children,\\n    spinnerSize = 'small',\\n    spinner = <Spinner color='current' size={spinnerSize} />,\\n    isLoading,\\n    disableRipple = false,\\n    getButtonProps,\\n    getRippleProps,\\n  } = useButton({\\n    ref,\\n    ...props,\\n  });\\n\\n  const { ripples } = getRippleProps();\\n\\n  return (\\n    <button ref={domRef} {...getButtonProps()} className='bg-green-600 hover:bg-green-700 text-white px-4 py-2 rounded-md'>\\n      {isLoading ? spinner : children}\\n      {!disableRipple && <Ripple ripples={ripples} />}\\n    </button>\\n  );\\n});\\n\\nEuclidButton.displayName = 'EuclidButton';\\n\\nexport default EuclidButton;\\n```\\n\\nThis custom `EuclidButton` component provides a green background, which aligns with the Euclid color scheme. It includes a ripple effect for user feedback and a spinner for loading states, making it suitable for blockchain transactions or analytics actions.\"}\n",
            "No errors found\n",
            "Num examples missing system message: 13\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 2, 12\n",
            "mean / median: 4.461538461538462, 4.0\n",
            "p5 / p95: 2.0, 7.600000000000001\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 89, 1595\n",
            "mean / median: 395.9230769230769, 305.0\n",
            "p5 / p95: 123.0, 735.2000000000003\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 58, 1439\n",
            "mean / median: 339.6923076923077, 236.0\n",
            "p5 / p95: 87.8, 649.8000000000002\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
            "Dataset has ~5147 tokens that will be charged for during training\n",
            "By default, you'll train for 7 epochs on this dataset\n",
            "By default, you'll be charged for ~36029 tokens\n",
            "Estimated cost for fine-tuning: approximately $0.29\n"
          ]
        }
      ],
      "source": [
        "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
        "\n",
        "# Next, we specify the data path and open the JSONL file\n",
        "\n",
        "data_path = r'C:\\Users\\91982\\Downloads\\gpt-finetune\\gpt-finetune\\fitness-json.jsonl'\n",
        "\n",
        "# Load dataset\n",
        "with open(data_path) as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# We can inspect the data quickly by checking the number of examples and the first item\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)\n",
        "\n",
        "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
        "\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")\n",
        "\n",
        "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
        "\n",
        "# Token counting functions\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
        "\n",
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
        "\n",
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "TARGET_EPOCHS = 4\n",
        "MIN_EPOCHS = 1\n",
        "MAX_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "# Calculate the estimated cost for fine-tuning\n",
        "cost_per_100k_tokens = 0.80  # Cost for every 100,000 tokens\n",
        "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 100000) * cost_per_100k_tokens\n",
        "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GME1BGqXNGMQ"
      },
      "outputs": [],
      "source": [
        "# Function to save the dataset as a JSONL file\n",
        "def save_to_jsonl(conversations, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for conversation in conversations:\n",
        "            json_line = json.dumps(conversation)\n",
        "            file.write(json_line + '\\n')\n",
        "\n",
        "# Specify the path where you want to save the JSONL file in your Google Drive\n",
        "jsonl_file_path = r'C:\\Users\\91982\\Downloads\\gpt-finetune\\gpt-finetune\\fitness-json-clean.jsonl'\n",
        "# Save the dataset to the specified file path\n",
        "save_to_jsonl(dataset, jsonl_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d403QpVVN3BU",
        "outputId": "a3999c8f-b3d1-472d-d903-a84f00bd50a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training file id: file-GqekqvYH2xsyyGPm6TzJFvYr\n"
          ]
        }
      ],
      "source": [
        "#Upload data for training\n",
        "training_file_name = r'C:\\Users\\91982\\Downloads\\gpt-finetune\\gpt-finetune\\fitness-json-clean.jsonl'\n",
        "\n",
        "training_response = openai.File.create(\n",
        "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
        ")\n",
        "training_file_id = training_response[\"id\"]\n",
        "\n",
        "#Gives training file id\n",
        "print(\"Training file id:\", training_file_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlZEZYGTOY-L",
        "outputId": "aebdacd5-d7ca-445f-f46e-2f44646f7ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-LFoeEvSFtqjlmkGEDITnrrWn\",\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"created_at\": 1730926579,\n",
            "  \"finished_at\": null,\n",
            "  \"fine_tuned_model\": null,\n",
            "  \"organization_id\": \"org-fPIL4OD7fqovoJ1j5UGPLjrC\",\n",
            "  \"result_files\": [],\n",
            "  \"status\": \"validating_files\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-GqekqvYH2xsyyGPm6TzJFvYr\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": \"auto\",\n",
            "    \"batch_size\": \"auto\",\n",
            "    \"learning_rate_multiplier\": \"auto\"\n",
            "  },\n",
            "  \"trained_tokens\": null,\n",
            "  \"error\": {},\n",
            "  \"user_provided_suffix\": \"ted\",\n",
            "  \"seed\": 833909406,\n",
            "  \"estimated_finish\": null,\n",
            "  \"integrations\": []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#Create Fine-Tuning Job(dont run)\n",
        "suffix_name = \"ted\"\n",
        "\n",
        "response = openai.FineTuningJob.create(\n",
        "    training_file=training_file_id,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    suffix=suffix_name,\n",
        ")\n",
        "\n",
        "job_id = response[\"id\"]\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xyzue2SPYaZ",
        "outputId": "4f8ac2ff-16c4-4f02-e756-fdc0d618d665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created fine-tuning job: ftjob-LFoeEvSFtqjlmkGEDITnrrWn\n",
            "Validating training file: file-GqekqvYH2xsyyGPm6TzJFvYr\n",
            "Files validated, moving job to queued state\n",
            "Fine-tuning job started\n"
          ]
        }
      ],
      "source": [
        "#list events as fine-tuning progresses(dont run)\n",
        "job_id=\"ftjob-LFoeEvSFtqjlmkGEDITnrrWn\"\n",
        "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
        "\n",
        "events = response[\"data\"]\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event[\"message\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUgVjlQxUMhw",
        "outputId": "981c8d40-e438-4ca5-8672-cdc30ab0b666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-LFoeEvSFtqjlmkGEDITnrrWn\",\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"created_at\": 1730926579,\n",
            "  \"finished_at\": 1730926936,\n",
            "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0125:personal:ted:AQhSM9sI\",\n",
            "  \"organization_id\": \"org-fPIL4OD7fqovoJ1j5UGPLjrC\",\n",
            "  \"result_files\": [\n",
            "    \"file-dVoTHdAHXDToVmyBwR04SOl1\"\n",
            "  ],\n",
            "  \"status\": \"succeeded\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-GqekqvYH2xsyyGPm6TzJFvYr\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": 7,\n",
            "    \"batch_size\": 1,\n",
            "    \"learning_rate_multiplier\": 2\n",
            "  },\n",
            "  \"trained_tokens\": 35847,\n",
            "  \"error\": {},\n",
            "  \"user_provided_suffix\": \"ted\",\n",
            "  \"seed\": 833909406,\n",
            "  \"estimated_finish\": null,\n",
            "  \"integrations\": []\n",
            "}\n",
            "\n",
            "Fine-tuned model id: ft:gpt-3.5-turbo-0125:personal:ted:AQhSM9sI\n"
          ]
        }
      ],
      "source": [
        "#retrieve fine-tune model id\n",
        "response = openai.FineTuningJob.retrieve(\"ftjob-LFoeEvSFtqjlmkGEDITnrrWn\")\n",
        "fine_tuned_model_id = \"ft:gpt-3.5-turbo-0125:personal:ted:AQhSM9sI\"\n",
        "\n",
        "print(response)\n",
        "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L7IEZScVi62",
        "outputId": "a418535f-b339-4889-f9a2-3702d2df9117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': \"You are an overly friendly UI graphic designer chatbot named DesignBuddy. Your goal is to help users create a basic UI guideline for their brand, and you're not satisfied unless the customer is completely satisfied.\"}, {'role': 'user', 'content': 'How can I design a sidebar for Euclid’s blockchain analytics dashboard?'}]\n"
          ]
        }
      ],
      "source": [
        "#Test it out!\n",
        "test_messages = []\n",
        "\n",
        "system_message = \"You are an overly friendly UI graphic designer chatbot named DesignBuddy. Your goal is to help users create a basic UI guideline for their brand, and you're not satisfied unless the customer is completely satisfied.\"\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = \"How can I design a sidebar for Euclid’s blockchain analytics dashboard?\"\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "print(test_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ2FtN_IVyxC",
        "outputId": "a60d6bb5-79c3-4744-9af4-9177fedcba91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Euclid’s blockchain analytics dashboard, let’s aim for a clean and professional sidebar. How about using Euclid’s brand colors for the background and accent elements? This will help users quickly identify the Euclid platform. Would you like me to create a mockup for you?\n"
          ]
        }
      ],
      "source": [
        "#OpenAI Chat Completions\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
        "    messages=test_messages,\n",
        "    temperature=0,\n",
        "    max_tokens= 500\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjRZ8-EAvg2K"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "TjYFpQkDWsQQ",
        "outputId": "547a76d0-9110-444b-f46c-eab0fcd55c64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7871\n",
            "* Running on public URL: https://458d4b6e4b399ae493.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://458d4b6e4b399ae493.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#run\n",
        "def generate_completion(user_prompt, chat_history):\n",
        "    # Initialize the system message and the list of messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an overly friendly UI graphic designer chatbot named DesignBuddy. Your goal is to help users create a basic UI guideline for Euclid, and you're not satisfied unless the customer is completely satisfied. Clour scheme includes pinkish red and light green. You provide code for all components in react and using nexus.\"}\n",
        "    ]\n",
        "\n",
        "    # Add chat history messages\n",
        "    for user_msg, _ in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "\n",
        "    # Add the latest user message\n",
        "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "    # Call the OpenAI API\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"ft:gpt-3.5-turbo-0125:personal:ted:AQhSM9sI\",\n",
        "        messages=messages,\n",
        "        max_tokens=500,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Return the generated response\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "def chat_bot(message, history):\n",
        "    # Ensure history is initialized as a list\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    # Append the user's message with an empty response\n",
        "    history.append((message, \"\"))\n",
        "\n",
        "    # Generate the assistant's response\n",
        "    response = generate_completion(message, history)\n",
        "\n",
        "    # Update the last entry in the history with the assistant's response\n",
        "    if history:\n",
        "        history[-1] = (history[-1][0], response)\n",
        "\n",
        "    # Return the updated chat history and state\n",
        "    return history, history\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as iface:\n",
        "    chat_input = gr.Textbox(label=\"Message\")\n",
        "    chat_history = gr.Chatbot(label=\"Chat History\")\n",
        "    state = gr.State([])  # Initialize with an empty list\n",
        "\n",
        "    # Bind the chat input to the chat_bot function and handle both outputs\n",
        "    chat_input.submit(chat_bot, inputs=[chat_input, state], outputs=[chat_history, state])\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7866\n",
            "* Running on public URL: https://b230f435ad247838ba.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b230f435ad247838ba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 2028, in process_api\n",
            "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1834, in postprocess_data\n",
            "    prediction_value = block.postprocess(prediction_value)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/components/chatbot.py\", line 531, in postprocess\n",
            "    self._check_format(value, \"messages\")\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/components/chatbot.py\", line 321, in _check_format\n",
            "    raise Error(\n",
            "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "\n",
        "# Function to generate a response from the model\n",
        "def generate_completion(user_prompt, chat_history):\n",
        "    # Initialize the system message and the list of messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an overly friendly UI graphic designer chatbot named DesignBuddy. Your goal is to help users create a basic UI guideline for their brand, and you're not satisfied unless the customer is completely satisfied.\"}\n",
        "    ]\n",
        "\n",
        "    # Add chat history messages to the messages list\n",
        "    for user_msg, assistant_msg in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "    # Add the latest user message\n",
        "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "    # Call the OpenAI API (replace \"fine_tuned_model_id\" with your actual model ID)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"ft:gpt-3.5-turbo-0125:personal:ted:AQhSM9sI\",\n",
        "        messages=messages,\n",
        "        max_tokens=500,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Return the generated response\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "# Function to handle the chat interaction and store history\n",
        "def chat_bot(user_message, history):\n",
        "    # Ensure history is initialized as a list\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    # Generate the assistant's response based on the user's message and chat history\n",
        "    assistant_response = generate_completion(user_message, history)\n",
        "\n",
        "    # Append the user's message and the assistant's response to the chat history\n",
        "    history.append((user_message, assistant_response))\n",
        "\n",
        "    # Return the updated chat history and state\n",
        "    return history, history\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as iface:\n",
        "    chat_input = gr.Textbox(label=\"Message\", placeholder=\"Type your message here...\")\n",
        "    chat_history = gr.Chatbot(label=\"Chat History\", type='messages')  # Specify `type='messages'`\n",
        "    state = gr.State([])  # Initialize with an empty list to store chat history\n",
        "\n",
        "    # Bind the chat input to the chat_bot function\n",
        "    chat_input.submit(chat_bot, inputs=[chat_input, state], outputs=[chat_history, state])\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Obtaining dependency information for streamlit from https://files.pythonhosted.org/packages/ef/e1/f9c479f9dbe0bb702ea5ca6608f10e91a708b438f7fb4572a2642718c6e3/streamlit-1.39.0-py2.py3-none-any.whl.metadata\n",
            "  Downloading streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting altair<6,>=4.0 (from streamlit)\n",
            "  Obtaining dependency information for altair<6,>=4.0 from https://files.pythonhosted.org/packages/9b/52/4a86a4fa1cc2aae79137cc9510b7080c3e5aede2310d14fae5486feec7f7/altair-5.4.1-py3-none-any.whl.metadata\n",
            "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
            "  Obtaining dependency information for blinker<2,>=1.0.0 from https://files.pythonhosted.org/packages/bb/2a/10164ed1f31196a2f7f3799368a821765c62851ead0e630ab52b8e14b4d0/blinker-1.8.2-py3-none-any.whl.metadata\n",
            "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting cachetools<6,>=4.0 (from streamlit)\n",
            "  Obtaining dependency information for cachetools<6,>=4.0 from https://files.pythonhosted.org/packages/a4/07/14f8ad37f2d12a5ce41206c21820d8cb6561b728e51fad4530dff0552a67/cachetools-5.5.0-py3-none-any.whl.metadata\n",
            "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (8.0.4)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (23.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (10.0.1)\n",
            "Collecting protobuf<6,>=3.20 (from streamlit)\n",
            "  Obtaining dependency information for protobuf<6,>=3.20 from https://files.pythonhosted.org/packages/1c/f2/baf397f3dd1d3e4af7e3f5a0382b868d25ac068eefe1ebde05132333436c/protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (8.2.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Obtaining dependency information for gitpython!=3.1.19,<4,>=3.0.7 from https://files.pythonhosted.org/packages/e9/bd/cc3a402a6439c15c3d4294333e13042b915bbeab54edc457c723931fed3f/GitPython-3.1.43-py3-none-any.whl.metadata\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Obtaining dependency information for pydeck<1,>=0.8.0b4 from https://files.pythonhosted.org/packages/ab/4c/b888e6cf58bd9db9c93f40d1c6be8283ff49d88919231afe93a6bcf61626/pydeck-0.9.1-py2.py3-none-any.whl.metadata\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from streamlit) (6.3.2)\n",
            "Requirement already satisfied: jinja2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.17.3)\n",
            "Collecting narwhals>=1.5.2 (from altair<6,>=4.0->streamlit)\n",
            "  Obtaining dependency information for narwhals>=1.5.2 from https://files.pythonhosted.org/packages/05/a8/22981ae479f9fc8d8f2d2bae3e399222887693c08fadeb575c3e220d1179/narwhals-1.12.1-py3-none-any.whl.metadata\n",
            "  Downloading narwhals-1.12.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/siddharthsingh/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
            "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.7/414.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-1.12.1-py3-none-any.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.1/195.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, protobuf, narwhals, cachetools, blinker, pydeck, gitdb, altair, gitpython, streamlit\n",
            "Successfully installed altair-5.4.1 blinker-1.8.2 cachetools-5.5.0 gitdb-4.0.11 gitpython-3.1.43 narwhals-1.12.1 protobuf-5.28.3 pydeck-0.9.1 smmap-5.0.1 streamlit-1.39.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-11-03 03:51:54.699 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-11-03 03:51:54.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import openai\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Streamlit app title\n",
        "st.title(\"Fine-Tuned Chatbot\")\n",
        "\n",
        "# Sidebar info\n",
        "st.sidebar.header(\"Chatbot Settings\")\n",
        "temperature = st.sidebar.slider(\"Temperature\", 0.0, 1.0, 0.5)\n",
        "max_tokens = st.sidebar.slider(\"Max Tokens\", 50, 500, 150)\n",
        "\n",
        "# Chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Function to interact with the OpenAI API using your fine-tuned model\n",
        "def generate_response(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        model=\"your-finetuned-model-id\",  # Replace with your fine-tuned model ID\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        stop=None  # Adjust stop sequences as needed\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# User input\n",
        "user_input = st.text_input(\"You:\", key=\"user_input\")\n",
        "\n",
        "# Display chat messages from the conversation history\n",
        "for message in st.session_state.messages:\n",
        "    st.write(f\"{message['role']}: {message['content']}\")\n",
        "\n",
        "# Handle user input and bot response\n",
        "if user_input:\n",
        "    # Append user message to the chat history\n",
        "    st.session_state.messages.append({\"role\": \"User\", \"content\": user_input})\n",
        "\n",
        "    # Generate bot response\n",
        "    response_text = generate_response(user_input)\n",
        "    st.session_state.messages.append({\"role\": \"Bot\", \"content\": response_text})\n",
        "\n",
        "    # Clear the user input\n",
        "    st.session_state.user_input = \"\"\n",
        "\n",
        "    # Display the new bot response\n",
        "    st.write(f\"Bot: {response_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "VBO1cGrP7wqe",
        "outputId": "3caac548-99b1-46b5-c500-6cd8948e71be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7862\n",
            "* Running on public URL: https://1bc6f79aba1ff48972.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://1bc6f79aba1ff48972.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 2018, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1567, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 846, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/var/folders/v7/r8zlw5hn5sl9y7xl99dpwqnh0000gn/T/ipykernel_8313/1385086685.py\", line 35, in chat_bot\n",
            "    response = generate_completion(message, history)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: generate_completion() takes 1 positional argument but 2 were given\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 2018, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1567, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/siddharthsingh/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 846, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/var/folders/v7/r8zlw5hn5sl9y7xl99dpwqnh0000gn/T/ipykernel_8313/1385086685.py\", line 35, in chat_bot\n",
            "    response = generate_completion(message, history)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: generate_completion() takes 1 positional argument but 2 were given\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Store conversation history\n",
        "conversation_history = []\n",
        "\n",
        "def generate_completion(user_prompt):\n",
        "    # Append user prompt to the conversation history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "    # Generate response from OpenAI\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"ft:gpt-3.5-turbo-0125:personal:ted:9rYh8DL6\",\n",
        "        messages=messages,\n",
        "        max_tokens=500,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract the assistant's message\n",
        "    assistant_message = response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "    # Append assistant's message to the conversation history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "    # Return the full conversation history as a single string\n",
        "    return \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in conversation_history])\n",
        "\n",
        "def clear_history():\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "    return \"\"\n",
        "\n",
        "# Create Gradio Blocks interface\n",
        "with gr.Blocks() as iface:\n",
        "    with gr.Row():\n",
        "        user_input = gr.Textbox(label=\"Enter your message\", placeholder=\"Type your message here...\")\n",
        "        submit_button = gr.Button(\"Send\")\n",
        "        clear_button = gr.Button(\"Clear History\")\n",
        "\n",
        "    conversation_output = gr.Textbox(label=\"Conversation History\", interactive=False, value=\"\")\n",
        "\n",
        "    def on_submit(user_message):\n",
        "        return generate_completion(user_message)\n",
        "\n",
        "    # Bind the submit button to the on_submit function\n",
        "    submit_button.click(on_submit, inputs=user_input, outputs=conversation_output)\n",
        "\n",
        "    # Bind the clear button to the clear_history function\n",
        "    clear_button.click(clear_history, inputs=[], outputs=conversation_output)\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQxyJgrptb1-"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/local/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
